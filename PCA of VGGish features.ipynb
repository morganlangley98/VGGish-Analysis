{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8de55c",
   "metadata": {},
   "source": [
    "# PCA analysis of VGGish features\n",
    "Follow VGGish script to extract features from each set of recordings and sites. For each 0.96s an 128 feature embedding will have been extracted. These relate to various features in the spectrogram.\n",
    "\n",
    "To model this data, dimensionality reduction is required. From prerequisite studies (UMAP vs PCA), UMAP excelled in visualisation. However, performed badly and the model residuals were all over the place. PCA to three components captured the majority of variation, and the model tests were much better aligned.\n",
    "\n",
    "After you have all the vggish features sorted, with time and date, into separate csv files move onto this notebook.\n",
    "\n",
    "Here we first combine the VGGish features into one DF, then drop the non-numeric columns, carry out PCA analysis, then add non-numeric columns back on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all mean CSV files and combine them into a single DataFrame\n",
    "def combine_mean_csv_files(folder_path):\n",
    "    # Create an empty list to store the DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('_mean.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the CSV file into a DataFrame\n",
    "            mean_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            dfs.append(mean_df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Specify the folder path where the mean CSV files are located\n",
    "mean_folder_path = '/mean_folder/'\n",
    "\n",
    "# Combine all mean CSV files into one big DataFrame\n",
    "big_df = combine_mean_csv_files(mean_folder_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(big_df)\n",
    "\n",
    "# Create the output folder to save the files if it doesn't exist\n",
    "output_folder = '/mean_combined/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the big DataFrame to a CSV file in the output folder\n",
    "#output_file_path = os.path.join(output_folder, 'combined_mean_values_10min.csv')\n",
    "big_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b271f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'SiteID'\n",
    "big_df = big_df.sort_values(by='SiteID')\n",
    "\n",
    "# Drop the specified columns\n",
    "df_1 = big_df.drop(columns=['date', 'time', 'SiteID', 'recording'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_1 contains your data\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_1)\n",
    "\n",
    "# Define the number of components for PCA\n",
    "n_components = 10\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Compute the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a scree plot with customizations\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, n_components + 1), explained_variance, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance Ratio', fontsize=14)\n",
    "plt.title('Scree Plot of VGGish Principal components analysis', fontsize=14)\n",
    "plt.xticks(np.arange(1, n_components + 1), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('scree_plot.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the data\n",
    "standardized_data = scaler.fit_transform(df_1)\n",
    "\n",
    "# Define the number of components for PCA\n",
    "n_components = 3\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "reduced_data = pca.fit_transform(standardized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the VGGish PCA outputs, with the additional non-numeric dat\n",
    "df = pd.DataFrame(reduced_data, columns=['VGGish_1', 'VGGish_2', 'VGGish_3'])\n",
    "df['Site_code'] = big_df['SiteID'].values\n",
    "df['Date'] = big_df['date'].values\n",
    "df['Time'] = big_df['time'].values\n",
    "df['Recording'] = big_df['recording'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aee086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data to visualise the relationship\n",
    "unique_labels = df['Site_code'].unique()\n",
    "color_map = plt.get_cmap('tab20')\n",
    "num_colors = len(unique_labels)\n",
    "colors = color_map(np.linspace(0, 1, num_colors))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    label_data = df[df['Site_code'] == label]\n",
    "    plt.scatter(label_data['VGGish_1'], label_data['VGGish_2'],\n",
    "                color=colors[i], label=label, s=0.5)\n",
    "    \n",
    "#Calculate centroid coordinates for each site\n",
    "centroids = df.groupby('Site_code')[['VGGish_1', 'VGGish_2']].mean().values\n",
    "\n",
    "# Plot the centroid markers on top\n",
    "for i, centroid in enumerate(centroids):\n",
    "    plt.scatter(centroid[0], centroid[1],\n",
    "                color=colors[i], marker='o',\n",
    "                edgecolor='black', linewidth=1, s=50)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('VGGish_1', size=15)\n",
    "plt.ylabel('VGGish_2', size=15)\n",
    "\n",
    "plt.text(0.03, 0.97, 'Site (PCA)', transform=plt.gca().transAxes,\n",
    "         fontsize=15, verticalalignment='top')\n",
    "\n",
    "\n",
    "plt.savefig('/Site_PCA.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e04256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe\n",
    "df.to_csv('/PCA_10min.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
