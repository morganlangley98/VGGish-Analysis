{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa805fa",
   "metadata": {},
   "source": [
    "# VGGish implementation\n",
    "\n",
    "This notebook will take you from raw audio, implement the VGGish model to extract an 128 feature embedding or each 0.96s of audio, then save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06612de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required dependancies for model\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features with VGGish, 128 for each 0.96s of respective wav or mp3 file.\n",
    "\n",
    "# The expected duration and samplerate of input to VGGish\n",
    "VGGISH_WINDOW_SIZE = 0.96\n",
    "VGGISH_SR = 16_000\n",
    "\n",
    "# The samplerate at which the audio is loaded\n",
    "# Set to None if you don't wish to resample the audio.\n",
    "SAMPLE_RATE = 16_000\n",
    "\n",
    "# Directory containing all the audio files to analyze\n",
    "AUDIO_DIR = Path(\"File_path\")\n",
    "\n",
    "# Load the model\n",
    "model = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
    "\n",
    "# Get all the WAV files in the AUDIO_DIR\n",
    "audio_files = AUDIO_DIR.glob(\"**/*.[wW][aA][vV]\")\n",
    "\n",
    "results = []\n",
    "for path in audio_files:\n",
    "    try:\n",
    "        # load the audio\n",
    "        waveform, sr = librosa.load(\n",
    "            str(path),\n",
    "            # If you specify `sr` it will resample the audio to the provided\n",
    "            # sampling rate.\n",
    "            sr=SAMPLE_RATE,\n",
    "        )\n",
    "    except (RuntimeError, EOFError):\n",
    "        # Some audio files might be corrupted. Ignore and go to the next one.\n",
    "        print(f\"The audio file {path} is corrupted\")\n",
    "        continue\n",
    "\n",
    "    # Run the model on the audio.\n",
    "    # VGGish will cut longer audios into 0.96 seconds @ 16kHz frames and\n",
    "    # process each individually.\n",
    "    embeddings = model(waveform)\n",
    "\n",
    "    # Each frame should have 128 features. Make sure this is the case.\n",
    "    embeddings.shape.assert_is_compatible_with([None, 128])\n",
    "\n",
    "    # If the samplerate is different from the default one the\n",
    "    # duration of audio that VGGish processes is no longer 0.96 seconds\n",
    "    # So we need this adjustment.\n",
    "    window_size = VGGISH_WINDOW_SIZE * (VGGISH_SR / sr)\n",
    "\n",
    "    # Store info of the embeddings of each frame\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "        results.append(\n",
    "            {\n",
    "                \"recording\": path,\n",
    "                \"embeddings\": embedding,\n",
    "                \"start_time\": window_size * index,\n",
    "                \"end_time\": window_size * (index + 1),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results as df\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results as a DF\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd610912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing redundant file labels so just features\n",
    "indices_features = results.drop(['start_time', 'end_time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = indices_features['recording']\n",
    "recordings_df = pd.DataFrame(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting embeddings into 128 columns (1 for each feature)\n",
    "features = np.stack(indices_features.embeddings)\n",
    "indices_features.loc[:, [f\"feat_{n}\" for n in range(128)]] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9908dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)\n",
    "site_ids = ['Site_id'] * (len(df))\n",
    "df['SiteID'] = site_ids\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5007854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the recording ID so AutoCorrelation effects can be seen\n",
    "merged_df = pd.merge(df, recordings_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adf752",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9096fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Path_to_file_name.CSV'\n",
    "merged_df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
